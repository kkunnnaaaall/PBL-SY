{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9374e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ML Model Trainer (using local CSV) ---\n",
      "Successfully loaded dataset from ..\\data\\transaction_dataset.csv\n",
      "Original columns: ['TxHash', 'BlockHeight', ' TimeStamp', 'From', 'To', 'Value', 'ContractAddress', 'Input', 'Class']\n",
      "Engineered features 'input_data_length' and 'is_contract_interaction'.\n",
      "Using Features: ['Value', 'input_data_length', 'is_contract_interaction']\n",
      "Using Target: Class\n",
      "Features selected and scaled.\n",
      "Data split: 93887 training, 23472 testing.\n",
      "Model training complete.\n",
      "\n",
      "--- Model Evaluation ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86     15989\n",
      "           1       0.69      0.81      0.74      7483\n",
      "\n",
      "    accuracy                           0.82     23472\n",
      "   macro avg       0.80      0.82      0.80     23472\n",
      "weighted avg       0.84      0.82      0.83     23472\n",
      "\n",
      "Model saved to phishing_classifier.pkl\n",
      "Scaler saved to feature_scaler.pkl\n",
      "\n",
      "SUCCESS! You can now run the backend API.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"--- ML Model Trainer (using local CSV) ---\")\n",
    "\n",
    "# --- 1. Define Paths ---\n",
    "DATA_PATH = os.path.join(\"..\", \"data\", \"transaction_dataset.csv\")\n",
    "MODEL_PATH = os.path.join(\"phishing_classifier.pkl\")\n",
    "SCALER_PATH = os.path.join(\"feature_scaler.pkl\")\n",
    "\n",
    "# --- 2. Load Dataset ---\n",
    "# TODO: Make sure your CSV file is in the /data/ folder\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Successfully loaded dataset from {DATA_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {DATA_PATH}\")\n",
    "    print(\"Please download/move your dataset and save it there.\")\n",
    "    # Stop execution if file isn't found\n",
    "    raise\n",
    "\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "# We create numerical features from your string-based columns\n",
    "# a. 'Value': Already a number, just ensure it's numeric\n",
    "df['Value'] = pd.to_numeric(df['Value'], errors='coerce').fillna(0)\n",
    "\n",
    "# b. 'Input': Get the length of the input data. Scams often have large/small inputs.\n",
    "#    .str.len() assumes it's a string, we'll handle '0x' prefix\n",
    "df['Input'] = df['Input'].astype(str)\n",
    "df['input_data_length'] = df['Input'].apply(lambda x: len(x.replace('0x', '')))\n",
    "\n",
    "# c. 'ContractAddress': Create a binary feature: 1 if it IS a contract interaction, 0 if not.\n",
    "df['is_contract_interaction'] = df['ContractAddress'].notna().astype(int)\n",
    "\n",
    "# d. 'Class': This is our target. Assuming 1 for Phishing, 0 for Safe.\n",
    "# TODO: Verify this! If 'Class' is \"Phishing\" and \"Safe\" strings, uncomment the next line\n",
    "# df['Class'] = df['Class'].apply(lambda x: 1 if x == 'Phishing' else 0)\n",
    "\n",
    "print(\"Engineered features 'input_data_length' and 'is_contract_interaction'.\")\n",
    "\n",
    "# --- 4. Feature Selection & Preprocessing ---\n",
    "# These are the \"inputs\" for the model. We use our *newly engineered* features.\n",
    "features = ['Value', 'input_data_length', 'is_contract_interaction']\n",
    "target = 'Class' # This is your 'is_phishing' column\n",
    "\n",
    "print(f\"Using Features: {features}\")\n",
    "print(f\"Using Target: {target}\")\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Features selected and scaled.\")\n",
    "\n",
    "# --- 5. Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print(f\"Data split: {len(X_train)} training, {len(X_test)} testing.\")\n",
    "\n",
    "# --- 6. Train the Model ---\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 7. Evaluate the Model ---\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- 8. Save the Model and Scaler ---\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")\n",
    "print(f\"Scaler saved to {SCALER_PATH}\")\n",
    "print(\"\\nSUCCESS! You can now run the backend API.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dapp-project",
   "language": "python",
   "name": "dapp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
